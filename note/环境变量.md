设置若干环境变量（一次设置，后面都用）


# 解决 head_dim 不整除 + 避免 flash kernel
export PT_SDPA_ENABLE_HEAD_DIM_PADDING=1
python - <<'PY'
import torch
try: torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)
except: pass
PY

# 让 Transformers 完全离线 & 加快解析
export TRANSFORMERS_OFFLINE=1
export HF_HUB_OFFLINE=1
export HF_DATASETS_OFFLINE=1

# 减少显存碎片（对大模型装载有帮助）
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
