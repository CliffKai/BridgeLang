设置若干环境变量（一次设置，后面都用）


# 解决 head_dim 不整除 + 避免 flash kernel
export PT_SDPA_ENABLE_HEAD_DIM_PADDING=1
python - <<'PY'
import torch
try: torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)
except: pass
PY

# 让 Transformers 完全离线 & 加快解析
export TRANSFORMERS_OFFLINE=1
export HF_HUB_OFFLINE=1
export HF_DATASETS_OFFLINE=1

# 减少显存碎片（对大模型装载有帮助）
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512


# 开启 FA2 的 head-dim padding（两个名字都设上，兼容不同编译）
export ENABLE_FLASH_ATTENTION_WITH_HEAD_DIM_PADDING=1
export FLASH_ATTENTION_WITH_HEAD_DIM_PADDING=1

# 这个你之前已设过，但留着没坏处（针对 PyTorch SDPA 的兜底）
export PT_SDPA_ENABLE_HEAD_DIM_PADDING=1

